<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Vihang Patil</title>

  <meta name="author" content="Vihang Patil">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Vihang Patil</name>
              </p>
              <p>Hi, I am Vihang Patil a PhD student under <a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Sepp Hochreiter</a> at <a href="https://ml-jku.github.io/">Institute for Machine Learning, Linz</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:patil@ml.jku.at">Email</a> &nbsp/&nbsp
                <a href="data/VihangPatil-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=1iwYpk0AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/wehungpatil">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/vihangp">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/VihangPatil.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/VihangPatil.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research revolves aroung long term credit assignment in Reinforcement Learning and how we can build algorithms which learn to build abstractions to learn faster and generalise to unknown environments.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/mhnrudder.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://offline-rl-neurips.github.io/2021/pdf/15.pdf">
            <papertitle>Modern Hopfield Networks for Sample-Efficient Return Decomposition from Demonstrations</papertitle>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?user=eMcniXgAAAAJ&hl=en">Michael Widrich</a>,
          <a href="https://scholar.google.com/citations?user=FD27EMIAAAAJ&hl=en">Markus Hofmarcher</a>,
          <strong>Vihang Patil</strong>,
          <a href="https://scholar.google.com/citations?user=6f4h-xgAAAAJ&hl=en">Angela Bitto-Nemling</a>,
          <a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Sepp Hochreiter</a>
          <br>
      <em>Offline RL Workshop, Neurips</em>, 2021
          <br>
          <a href="https://www.youtube.com/watch?v=RXCWyWpg5YE">video</a>
          <p></p>
          <p>We introduce modern Hopfield networks for return decomposition for delayed rewards (Hopfield-RUDDER). We experimentally show that Hopfield-RUDDER is able to outperform LSTM-based RUDDER on various 1D environments with small numbers of episodes. 
          </p>
        </td>
          </tr>
<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src='images/offlinerl.png' width="160">
  </td>

  <td style="padding:20px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/abs/2111.04714">
  <papertitle>Understanding the Effects of Dataset Characteristics on Offline Reinforcement Learning</papertitle>
</a>
<br>
Kajetan Schweighofer,
<a href="https://scholar.google.com/citations?user=FD27EMIAAAAJ&hl=en">Markus Hofmarcher</a>,
<a href="https://scholar.google.com/citations?user=vh39WvYAAAAJ&hl=en">Marius-Constantin Dinu</a>,
<a href="https://scholar.google.at/citations?user=DMiTarMAAAAJ&hl=en">Philipp Renz</a>,
<a href="https://scholar.google.com/citations?user=6f4h-xgAAAAJ&hl=en">Angela Bitto-Nemling</a>,
<strong>Vihang Patil</strong>,
<a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Sepp Hochreiter</a>
<br>
<em>Ecological RL Workshop (Accepted for Oral), Neurips</em>, 2021
<br>
<a href="https://arxiv.org/abs/2111.04714">arXiv</a>
/
<a href="https://recorder-v3.slideslive.com/?share=54252&s=8437e42b-a0ea-4669-8760-972c96b75900">video</a>
/
<a href="https://github.com/ml-jku/OfflineRL">code</a>
<p></p>
<p>We conducted a comprehensive empirical analysis of how dataset characteristics effect the performance of Offline RL algorithms for discrete action environments.
</p>
</td>


</tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/minecraft.gif' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://ml-jku.github.io/blog/2020/align-rudder/">
      <papertitle>Align-RUDDER: Learning from Few Demonstrations by Reward Redistribution</papertitle>
    </a>
    <br>
    <strong>Vihang Patil</strong>,
    <a href="https://scholar.google.com/citations?user=FD27EMIAAAAJ&hl=en">Markus Hofmarcher</a>,
    <a href="https://scholar.google.com/citations?user=vh39WvYAAAAJ&hl=en">Marius-Constantin Dinu</a>,
    <a href="https://scholar.google.at/citations?user=NuwGRl8AAAAJ&hl=en">Matthias Dorfer</a>,
    <a href="https://scholar.google.at/citations?user=1PmRuNsAAAAJ&hl=en">Patrick M. Blies</a>,
    <a href="https://scholar.google.com/citations?user=KiRvOHcAAAAJ&hl=en">Johannes Brandstetter</a>,
    <a href="https://scholar.google.com/citations?user=5_n6NzwAAAAJ&hl=en">Jose A. Arjona-Medina</a>,
    <a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Sepp Hochreiter</a>
    <br>
<em>arXiv</em>, 2020
    <br>
    <a href="https://ml-jku.github.io/blog/2020/align-rudder/">blog</a>
/
    <a href="https://arxiv.org/abs/2009.14108">arXiv</a>
/
    <a href="https://www.youtube.com/watch?v=HO-_8ZUl-UY">video</a>
/
    <a href="https://github.com/ml-jku/align-rudder">code</a>
    <p></p>
    <p>We present Align-RUDDER an algorithm which learns from as few as two demonstrations. It does this by aligning demonstrations and speeds up learning by reducing the delay in reward.
    </p>
  </td>
    </tr>

  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src='images/ges.png' width="160">
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
  <a href="https://thesis.bul.sbu.usi.ch/theses/1725-1819Patil/pdf?1574252153">
    <papertitle>Guided Search for Maximum Entropy Reinforcement Learning</papertitle>
  </a>
  <br>
  <strong>Vihang Patil</strong>
  <br>
 2019
  <br>
  <p></p>
  <p>We propose a new convergent hybrid method which utilizes policy gradient directions to search in a smaller sub-space, called Guided Evolution Strategies with Sufficient Increase.
  </p>
</td>
  </tr>

</tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <p>
                Modified from Jon Barron's <a href="https://jonbarron.info/"> website.</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
