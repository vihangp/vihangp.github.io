<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Vihang Patil</title>

  <meta name="author" content="Vihang Patil">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-G9C9MSZ9ZL"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-G9C9MSZ9ZL');
</script>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Vihang Patil</name>
              </p>
              <p>Hi, I am Vihang Patil a PhD student under <a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Prof. Sepp Hochreiter</a> at <a href="https://ml-jku.github.io/">Institute for Machine Learning, Linz</a>. 
                <br>
                <br>
                My research revolves around long-term credit assignments in Reinforcement Learning and how we can build algorithms that learn to build abstractions faster and generalize to unknown parts of the environment. 
              </p>
              <p style="text-align:center">
                <a href="mailto:patil@ml.jku.at">Email</a> &nbsp/&nbsp
                <a href="data/VihangPatil-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=1iwYpk0AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/wehungpatil">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/vihangp">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/VihangPatil.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/VihangPatil.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News/Updates</heading>
            <ul>
              <li>Our <a href="https://arxiv.org/abs/2207.05742">paper</a> on Reactive Exploration to Cope with Non-Stationarity in Lifelong Reinforcement Learning has been accepted at COLLAS 2022. (20th May 2022)</li>
              <li>Our <a href="https://arxiv.org/abs/2111.04714">paper</a> on A Dataset Perspective on Offline Reinforcement Learning has been accepted at COLLAS 2022. (20th May 2022)</li>
              <li>Our <a href="https://proceedings.mlr.press/v162/paischer22a.html">paper</a> on History Compression via Language Models in Reinforcement Learning has been accepted at ICML 2022. (15th May 2022)</li>
              <li>Our <a href="https://proceedings.mlr.press/v162/patil22a.html">paper</a> on Align-RUDDER: Learning from few Demonstrations by Reward Redistribution has been accepted at ICML 2022 for a *long presentation*. (<2%) (15th May 2022)</li>
              <li>Our <a href="https://proceedings.mlr.press/v151/diouane22a.html">paper</a> on A Globally Convergent Evolutionary Strategy for Stochastic Constrained Optimization has been accepted at AISTATS, 2022.</li>
              <li>Worked at <a href="https://www.amazon.science">Amazon</a> as an Applied Science Intern. (Seattle, January - May 2022)</li>
            </ul>              
          </td>
        </tr>
      </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/minecraft.gif' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://proceedings.mlr.press/v162/patil22a.html">
      <papertitle>Align-RUDDER: Learning from Few Demonstrations by Reward Redistribution</papertitle>
    </a>
    <br>
    <strong>Vihang Patil</strong>,
    <a href="https://scholar.google.com/citations?user=FD27EMIAAAAJ&hl=en">Markus Hofmarcher</a>,
    <a href="https://scholar.google.com/citations?user=vh39WvYAAAAJ&hl=en">Marius-Constantin Dinu</a>,
    <a href="https://scholar.google.at/citations?user=NuwGRl8AAAAJ&hl=en">Matthias Dorfer</a>,
    <a href="https://scholar.google.at/citations?user=1PmRuNsAAAAJ&hl=en">Patrick M. Blies</a>,
    <a href="https://scholar.google.com/citations?user=KiRvOHcAAAAJ&hl=en">Johannes Brandstetter</a>,
    <a href="https://scholar.google.com/citations?user=5_n6NzwAAAAJ&hl=en">Jose A. Arjona-Medina</a>,
    <a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Sepp Hochreiter</a>
    <br>
<em>International Conference on Machine Learning (ICML)</em>, 2022
    <br>
    <a href="https://ml-jku.github.io/align-rudder/">blog</a>
/
    <a href="https://arxiv.org/abs/2009.14108">arXiv</a>
/
    <a href="https://www.youtube.com/watch?v=HO-_8ZUl-UY">video</a>
/
    <a href="https://github.com/ml-jku/align-rudder">code</a>
    <p></p>
    <p>We present Align-RUDDER an algorithm which learns from as few as two demonstrations. It does this by aligning demonstrations and speeds up learning by reducing the delay in reward. (Long presentation at ICML, < 2%)
    </p>
  </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/helm.gif' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://ml-jku.github.io/blog/2022/helm/">
      <papertitle>History Compression via Language Models in Reinforcement Learning</papertitle>
    </a>
    <br>
    <a href="https://scholar.google.com/citations?user=zdm5ZKwAAAAJ&hl=en">Fabian Paischer</a>,
    <a href="https://scholar.google.at/citations?user=R6p_vo4AAAAJ&hl=en">Thomas Adler</a>,
    <strong>Vihang Patil</strong>,
    <a href="https://scholar.google.com/citations?user=518MXv8AAAAJ&hl=de">Markus Holzleitner</a>,
    <a href="https://scholar.google.com/citations?user=6f4h-xgAAAAJ&hl=de">Angela Bitto-Nemling</a>,
    Sebastian Lehner,
    <a href="https://scholar.google.com/citations?user=-yGxzA4AAAAJ&hl=en">Hamid Eghbal-Zadeh</a>,
    <a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Sepp Hochreiter</a>
    <br>
<em>International Conference on Machine Learning (ICML)</em>, 2022
    <br>
    <a href="https://ml-jku.github.io/blog/2022/helm/">blog</a>
/
    <a href="https://proceedings.mlr.press/v162/paischer22a.html">arXiv</a>
/
    <a href="https://github.com/ml-jku/helm">code</a>
    <p></p>
    <p>HELM (History comprEssion via Language Models) is a novel framework for Reinforcement Learning (RL) in partially observable environments. Language is inherently well suited for abstraction and passing on experiences from one human to another. Therefore, we leverage a frozen pretrained language Transformer (PLT) to create abstract history representations for RL. (Spotlight presentation at ICML)
    </p>
  </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/nonstationary.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://ml-jku.github.io/blog/2022/helm/">
      <papertitle>Reactive Exploration to Cope with Non-Stationarity in Lifelong Reinforcement Learning</papertitle>
    </a>
    <br>
    <a href="https://www.jku.at/en/institute-of-computer-graphics/about-us/team/christian-steinparz/">Christian Steinparz</a>,
    <a href="https://scholar.google.de/citations?user=tgw-vEMAAAAJ&hl=de">Thomas Schmied</a>,
    <a href="https://scholar.google.com/citations?user=zdm5ZKwAAAAJ&hl=en">Fabian Paischer</a>,
    <a href="https://scholar.google.com/citations?user=vh39WvYAAAAJ&hl=en">Marius-Constantin Dinu</a>,
    <strong>Vihang Patil</strong>,
    <a href="https://scholar.google.com/citations?user=6f4h-xgAAAAJ&hl=de">Angela Bitto-Nemling</a>,
    <a href="https://scholar.google.com/citations?user=-yGxzA4AAAAJ&hl=en">Hamid Eghbal-Zadeh</a>,
    <a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Sepp Hochreiter</a>
    <br>
<em>Conference on Lifelong Learning (COLLAS)</em>, 2022
    <br>
    <a href="https://arxiv.org/abs/2207.05742">arXiv</a>
/
    <a href="https://github.com/ml-jku/reactive-exploration">code</a>
    <p></p>
    <p>We propose Reactive Exploration to track and react to continual domain shifts in lifelong reinforcement learning, and to update the policy correspondingly. 
    </p>
  </td>
    </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/offlinerl.png' width="160">
      </td>
    
      <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2111.04714">
      <papertitle>A Dataset Perspective on Offline Reinforcement Learning</papertitle>
    </a>
    <br>
    Kajetan Schweighofer,
    Andreas Radler,
    <a href="https://scholar.google.com/citations?user=vh39WvYAAAAJ&hl=en">Marius-Constantin Dinu</a>,
    <a href="https://scholar.google.com/citations?user=FD27EMIAAAAJ&hl=en">Markus Hofmarcher</a>,
    <strong>Vihang Patil</strong>,
    <a href="https://scholar.google.com/citations?user=6f4h-xgAAAAJ&hl=en">Angela Bitto-Nemling</a>,
    <a href="https://scholar.google.com/citations?user=-yGxzA4AAAAJ&hl=en">Hamid Eghbal-Zadeh</a>,
    <a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Sepp Hochreiter</a>
    <br>
    <em>Conference on Lifelong Learning (COLLAS)</em>, 2022
    <br>
    <a href="https://arxiv.org/abs/2111.04714">arxiv</a>
    /
    <a href="https://github.com/ml-jku/OfflineRL">code</a>
    <p></p>
    <p>We conducted a comprehensive empirical analysis of how dataset characteristics effect the performance of Offline RL algorithms for discrete action environments.
    </p>
    </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/pcces.png' width="160">
      </td>
    
      <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://proceedings.mlr.press/v151/diouane22a.html">
      <papertitle>A Globally Convergent Evolutionary Strategy for Stochastic Constrained Optimization with Applications to Reinforcement Learning</papertitle>
    </a>
    <br>
    <a href="https://scholar.google.com/citations?user=TaYmtf0AAAAJ&hl=en">Youssef Diouane</a>*,
    <a href="https://scholar.google.com/citations?user=V1ONSgIAAAAJ&hl=en">Aurelien Lucchi</a>*,
    <strong>Vihang Patil</strong>*
    <br>
    <em>International Conference on Artificial Intelligence and Statistics (AISTATS)</em>, 2022
    <br>
    <a href="https://proceedings.mlr.press/v151/diouane22a.html">arXiv</a>
    <p></p>
    <p>In this work, we design a novel optimization algorithm with a sufficient decrease mechanism that ensures convergence and that is based only on estimates of the functions. We demonstrate the applicability of this algorithm on two types of experiments: i) a control task for maximizing rewards and ii) maximizing rewards subject to a non-relaxable set of constraints. (*equal contribution)
    </p>
    </td>
    </tr>





<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src='images/mhnrudder.png' width="160">
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
<a href="https://offline-rl-neurips.github.io/2021/pdf/15.pdf">
  <papertitle>Modern Hopfield Networks for Sample-Efficient Return Decomposition from Demonstrations</papertitle>
</a>
<br>
<a href="https://scholar.google.com/citations?user=eMcniXgAAAAJ&hl=en">Michael Widrich</a>,
<a href="https://scholar.google.com/citations?user=FD27EMIAAAAJ&hl=en">Markus Hofmarcher</a>,
<strong>Vihang Patil</strong>,
<a href="https://scholar.google.com/citations?user=6f4h-xgAAAAJ&hl=en">Angela Bitto-Nemling</a>,
<a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Sepp Hochreiter</a>
<br>
<em>Offline RL Workshop, Neurips</em>, 2021
<br>
<a href="https://www.youtube.com/watch?v=RXCWyWpg5YE">video</a>
<p></p>
<p>We introduce modern Hopfield networks for return decomposition for delayed rewards (Hopfield-RUDDER). We experimentally show that Hopfield-RUDDER is able to outperform LSTM-based RUDDER on various 1D environments with small numbers of episodes. 
</p>
</td>
</tr>



  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src='images/ges.png' width="160">
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
  <a href="https://thesis.bul.sbu.usi.ch/theses/1725-1819Patil/pdf?1574252153">
    <papertitle>Guided Search for Maximum Entropy Reinforcement Learning</papertitle>
  </a>
  <br>
  <strong>Vihang Patil</strong>
  <br>
 2019
  <br>
  <p></p>
  <p>We propose a new convergent hybrid method which utilizes policy gradient directions to search in a smaller sub-space, called Guided Evolution Strategies with Sufficient Increase.
  </p>
</td>
  </tr>

</tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <p>
                Modified from Jon Barron's <a href="https://jonbarron.info/"> website.</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
