<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Vihang Patil</title>

  <meta name="author" content="Vihang Patil">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-G9C9MSZ9ZL"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-G9C9MSZ9ZL');
</script>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Vihang Patil</name>
              </p>
              <p>Hi, I am Vihang Patil an Applied Scientist at Amazon. I work in the team RUFUS (chatbot for shopping). 
                <br>
                <br>
                My current research interest is in 1) aligning large language models (LLMs) with human preferences, particularly through the use of reinforcement learning. 2) Sub-quadratic architectures for generative models.
                <br>
                <br>
                Previously, I was a Ph.D. student under <a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Prof. Sepp Hochreiter</a> at <a href="https://ml-jku.github.io/">Institute for Machine Learning, Linz</a>. 
                My research during my Phd revolved around long-term credit assignments in Reinforcement Learning and how we can build algorithms that build abstractions to learn faster and generalize to unknown parts of the environment. 
              </p>
              <p style="text-align:center">
                <a href="mailto:patil@ml.jku.at">Email</a> &nbsp/&nbsp
                <a href="data/VihangPatil-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=1iwYpk0AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/wehungpatil">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/vihangp">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/VihangPatil.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/VihangPatil.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News/Updates</heading>
            <ul>
              <li>Joined Amazon's Team RUFUS!</li>
              <li>Graduated my Phd with distinction!</li>              
              <li>New pre print on arxiv: <a href="https://arxiv.org/abs/2410.22391"> A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks</a> </li>              
              <li>New pre print on arxiv: <a href="https://arxiv.org/abs/2410.07071"> Retrieval-Augmented Decision Transformer: External Memory for In-context RL</a> </li>              
              <li>Paper accepted at the Conference in Lifelong Learning, 2024.</li>
              <li>I got married to the lovely <a href="https://yangchenroy.github.io">Yangchen Roy</a>.</li>
              <li>Paper accepted at the Generalisation in Planning workshop @ Neurips 2023.</li>
              <li>Worked at <a href="https://www.amazon.science">Amazon</a>, Seattle (September - December, 2023).</li>
              <li>We just won the <a href="https://sites.google.com/view/myochallenge">MyoChallenge @ Neurips2022</a>. Joint work with Rahul Siripurapa, Luis Ferro and others at IARAI and JKU.</li>
              <li>Paper accepted at the Deep RL workshop @ Neurips 2022.</li>
              <li>Our <a href="https://arxiv.org/abs/2207.05742">paper</a> on Reactive Exploration to Cope with Non-Stationarity in Lifelong Reinforcement Learning has been accepted at COLLAS 2022. (20th May 2022)</li>
              <li>Our <a href="https://arxiv.org/abs/2111.04714">paper</a> on A Dataset Perspective on Offline Reinforcement Learning has been accepted at COLLAS 2022. (20th May 2022)</li>
              <li>Our <a href="https://proceedings.mlr.press/v162/paischer22a.html">paper</a> on History Compression via Language Models in Reinforcement Learning has been accepted at ICML 2022. (15th May 2022)</li>
              <li>Our <a href="https://proceedings.mlr.press/v162/patil22a.html">paper</a> on Align-RUDDER: Learning from few Demonstrations by Reward Redistribution has been accepted at ICML 2022 for a *long presentation*. (<2%) (15th May 2022)</li>
              <li>Our <a href="https://proceedings.mlr.press/v151/diouane22a.html">paper</a> on A Globally Convergent Evolutionary Strategy for Stochastic Constrained Optimization has been accepted at AISTATS, 2022.</li>
              <li>Worked at <a href="https://www.amazon.science">Amazon</a> as an Applied Science Intern. (Seattle, January - May 2022)</li>
            </ul>              
          </td>
        </tr>
      </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/xlstm_action.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2410.22391">
            <papertitle>A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks</papertitle>
          </a>
          <br>
          <a href="https://scholar.google.de/citations?user=tgw-vEMAAAAJ&hl=de">Thomas Schmied</a>,
          <a href="https://scholar.google.at/citations?user=R6p_vo4AAAAJ&hl=en">Thomas Adler</a>,          
          <strong>Vihang Patil</strong>,
          <a href="https://scholar.google.com/citations?user=_YcZWcYAAAAJ&hl=en">Maximillian Beck</a>,
          <a href="https://scholar.google.com/citations?user=sBrtrxwAAAAJ&hl=en">Korbinian PÃ¶ppel</a>,
          <a href="https://scholar.google.com/citations?user=KiRvOHcAAAAJ&hl=en">Johannes Brandstetter</a>,
          <a href="https://scholar.google.at/citations?user=rb2AvxIAAAAJ&hl=en">Gunter Klambauer</a>,                              
          <a href="https://scholar.google.ca/citations?user=6nKHDKYAAAAJ&hl=en">Razvan Pascanu</a>,
          <a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Sepp Hochreiter</a>
          <br>
        <em>International Conference on Machine Learning (ICML)</em>, 2022
          <br>
          <a href="https://arxiv.org/pdf/2410.22391">pdf</a>
          <p></p>
          <p>We propose a Large Recurrent Action Model (LRAM) with an xLSTM at its core that comes with linear-time inference complexity and natural sequence length extrapolation abilities. Experiments on 432 tasks from 6 domains show that LRAM compares favorably to Transformers in terms of performance and speed.
          </p>
        </td>
          </tr>          
 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/decision_transformer.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2410.07071">
            <papertitle>Retrieval-Augmented Decision Transformer: External Memory for In-context RL</papertitle>
          </a>
          <br>
          <a href="https://scholar.google.de/citations?user=tgw-vEMAAAAJ&hl=de">Thomas Schmied</a>,
          <a href="https://scholar.google.com/citations?user=zdm5ZKwAAAAJ&hl=en">Fabian Paischer</a>,          
          <strong>Vihang Patil</strong>,
          <a href="https://scholar.google.com/citations?user=FD27EMIAAAAJ&hl=en">Markus Hofmarcher</a>,
          <a href="https://scholar.google.ca/citations?user=6nKHDKYAAAAJ&hl=en">Razvan Pascanu</a>,
          <a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Sepp Hochreiter</a>
          <br>
      <em>Under Review</em>
          <br>
          <a href="https://arxiv.org/pdf/2410.07071">pdf</a>
          <p></p>
          <p>We introduce Retrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external memory mechanism to store past experiences from which it retrieves only sub-trajectories relevant for the current situation. The retrieval component in RA-DT does not require training and can be entirely domain-agnostic. We evaluate the capabilities of RA-DT on grid-world environments, robotics simulations, and procedurally-generated video games.
          </p>
        </td>
          </tr>


          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/minimo.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openreview.net/forum?id=oMkUQKfsCU">
            <papertitle>Contrastive Abstraction for Reinforcement Learning</papertitle>
          </a>
          <br>
          <strong>Vihang Patil</strong>,
          <a href="https://scholar.google.com/citations?user=FD27EMIAAAAJ&hl=en">Markus Hofmarcher</a>,
          <a href="https://scholar.google.com/citations?user=0AUdatYAAAAJ&hl=en">Elisabeth Rumetshofer</a>,
          <a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Sepp Hochreiter</a>
          <br>
      <em>Generalisation in Planning workshop @ Neurips</em>, 2023
          <br>
          <a href="https://openreview.net/pdf?id=oMkUQKfsCU">pdf</a>
          <p></p>
          <p>We propose contrastive abstraction learning to find abstract states, where we assume that successive states in a trajectory belong to the same abstract state. Such abstract states may be basic locations, achieved subgoals, inventory, or health conditions. Contrastive abstraction learning first constructs clusters of state representations by contrastive learning and then applies modern Hopfield networks to determine the abstract states.
          </p>
        </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/samp.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>Simplified Priors for Object-Centric Learning</papertitle>
          </a>
          <br>
          <strong>Vihang Patil</strong>,
          <a href="https://scholar.google.com/citations?user=7H5n9VsAAAAJ&hl=en">Andreas Radler</a>,
          <a href="https://scholar.google.at/citations?user=J5Odv8wAAAAJ&hl=en">Daniel Klotz</a>,
          <a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Sepp Hochreiter</a>
          <br>
      <em>COLLAS</em>, 2024
          <br>
          <p></p>
          <p>We propose a non-iterative object-centric learning method using common building blocks, namely CNN, MaxPool and a modified Cross-Attention layer. A vastly modified version of this work is under review at Collas.
          </p>
        </td>
          </tr>          


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/myochallenge.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://proceedings.mlr.press/v220/caggiano23a">
            <papertitle>MyoChallenge 2022: Learning contact-rich manipulation using a musculoskeletal hand</papertitle>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?user=lCt9zVkAAAAJ&hl=en">Vittorio Caggiano</a>,                  
          Guillaume Durandau, Huwawei Wang, Alberto Chiappa, Alexander Mathis, Pablo Tano, Nisheet Patel, Alexandre Pouget, Pierre Schumacher, Georg Martius, Daniel Haeufle, Yiran Geng, Boshi An, Yifan Zhong, Jiaming Ji, Yuanpei Chen, Hao Dong, Yaodong Yang, Rahul Siripurapu, Luis Eduardo Ferro Diez, Michael Kopp, Vihang Patil, Sepp Hochreiter, 
          <a href="https://scholar.google.com/citations?user=v0N6rcMAAAAJ&hl=en">Rahul Siripurapu</a>,        
          <strong>Vihang Patil</strong>,
          <a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Sepp Hochreiter</a>,
          Yuval Tassa, Josh Merel, Randy Schultheis, Seungmoon Song, Massimo Sartori, 
          <a href="https://scholar.google.com/citations?user=lS96SqoAAAAJ&hl=en">Vikash Kumar</a>
          <br>
      <em>Neural Information Processing Systems (NeurIPS)</em>, 2022
          <br>
          <a href="https://proceedings.mlr.press/v220/caggiano23a/caggiano23a.pdf">pdf</a>
          <p></p>
          <p>In the MyoChallenge at the NeurIPS 2022 competition track, the task was to develop controllers for a realistic hand to solve a series of dexterous manipulation tasks. This work paper the challenge and its solutions. Our method was a co-winner of the Baoding Balls task.
          </p>
        </td>
          </tr>          

          <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/infodist.png' width="160">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openreview.net/forum?id=9CvMkA8oi8O">
          <papertitle>InfODist: Online distillation with Informative rewards improves generalization in Curriculum Learning</papertitle>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?user=v0N6rcMAAAAJ&hl=en">Rahul Siripurapu</a>,        
        <strong>Vihang Patil</strong>,Kajetan Schweighofer, Marius-Constantin Dinu, Thomas Schmied, Luis Eduardo Ferro Diez, Markus Holzleitner, Hamid Eghbal-zadeh, Michael K Kopp,
        <a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Sepp Hochreiter</a>
        <br>
    <em>Deep Reinforcement Learning Workshop, Neurips</em>, 2022
        <br>
        <a href="https://openreview.net/forum?id=9CvMkA8oi8O">openreview</a>
        <p></p>
        <p>A method to improve generalization in curriculum learning and an analysis of various factors affecting generalization.
        </p>
      </td>
        </tr>          


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/minecraft.gif' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://proceedings.mlr.press/v162/patil22a.html">
      <papertitle>Align-RUDDER: Learning from Few Demonstrations by Reward Redistribution</papertitle>
    </a>
    <br>
    <strong>Vihang Patil</strong>,
    <a href="https://scholar.google.com/citations?user=FD27EMIAAAAJ&hl=en">Markus Hofmarcher</a>,
    <a href="https://scholar.google.com/citations?user=vh39WvYAAAAJ&hl=en">Marius-Constantin Dinu</a>,
    <a href="https://scholar.google.at/citations?user=NuwGRl8AAAAJ&hl=en">Matthias Dorfer</a>,
    <a href="https://scholar.google.at/citations?user=1PmRuNsAAAAJ&hl=en">Patrick M. Blies</a>,
    <a href="https://scholar.google.com/citations?user=KiRvOHcAAAAJ&hl=en">Johannes Brandstetter</a>,
    <a href="https://scholar.google.com/citations?user=5_n6NzwAAAAJ&hl=en">Jose A. Arjona-Medina</a>,
    <a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Sepp Hochreiter</a>
    <br>
<em>International Conference on Machine Learning (ICML)</em>, 2022
    <br>
    <a href="https://ml-jku.github.io/align-rudder/">blog</a>
/
    <a href="https://arxiv.org/abs/2009.14108">arXiv</a>
/
    <a href="https://www.youtube.com/watch?v=HO-_8ZUl-UY">video</a>
/
    <a href="https://github.com/ml-jku/align-rudder">code</a>
    <p></p>
    <p>We present Align-RUDDER an algorithm which learns from as few as two demonstrations. It does this by aligning demonstrations and speeds up learning by reducing the delay in reward. (Long presentation at ICML, < 2%)
    </p>
  </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/helm.gif' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://ml-jku.github.io/blog/2022/helm/">
      <papertitle>History Compression via Language Models in Reinforcement Learning</papertitle>
    </a>
    <br>
    <a href="https://scholar.google.com/citations?user=zdm5ZKwAAAAJ&hl=en">Fabian Paischer</a>,
    <a href="https://scholar.google.at/citations?user=R6p_vo4AAAAJ&hl=en">Thomas Adler</a>,
    <strong>Vihang Patil</strong>,
    <a href="https://scholar.google.com/citations?user=518MXv8AAAAJ&hl=de">Markus Holzleitner</a>,
    <a href="https://scholar.google.com/citations?user=6f4h-xgAAAAJ&hl=de">Angela Bitto-Nemling</a>,
    Sebastian Lehner,
    <a href="https://scholar.google.com/citations?user=-yGxzA4AAAAJ&hl=en">Hamid Eghbal-Zadeh</a>,
    <a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Sepp Hochreiter</a>
    <br>
<em>International Conference on Machine Learning (ICML)</em>, 2022
    <br>
    <a href="https://ml-jku.github.io/blog/2022/helm/">blog</a>
/
    <a href="https://proceedings.mlr.press/v162/paischer22a.html">arXiv</a>
/
    <a href="https://github.com/ml-jku/helm">code</a>
    <p></p>
    <p>HELM (History comprEssion via Language Models) is a novel framework for Reinforcement Learning (RL) in partially observable environments. Language is inherently well suited for abstraction and passing on experiences from one human to another. Therefore, we leverage a frozen pretrained language Transformer (PLT) to create abstract history representations for RL. (Spotlight presentation at ICML)
    </p>
  </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/nonstationary.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://ml-jku.github.io/blog/2022/helm/">
      <papertitle>Reactive Exploration to Cope with Non-Stationarity in Lifelong Reinforcement Learning</papertitle>
    </a>
    <br>
    <a href="https://www.jku.at/en/institute-of-computer-graphics/about-us/team/christian-steinparz/">Christian Steinparz</a>,
    <a href="https://scholar.google.de/citations?user=tgw-vEMAAAAJ&hl=de">Thomas Schmied</a>,
    <a href="https://scholar.google.com/citations?user=zdm5ZKwAAAAJ&hl=en">Fabian Paischer</a>,
    <a href="https://scholar.google.com/citations?user=vh39WvYAAAAJ&hl=en">Marius-Constantin Dinu</a>,
    <strong>Vihang Patil</strong>,
    <a href="https://scholar.google.com/citations?user=6f4h-xgAAAAJ&hl=de">Angela Bitto-Nemling</a>,
    <a href="https://scholar.google.com/citations?user=-yGxzA4AAAAJ&hl=en">Hamid Eghbal-Zadeh</a>,
    <a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Sepp Hochreiter</a>
    <br>
<em>Conference on Lifelong Learning (COLLAS)</em>, 2022
    <br>
    <a href="https://arxiv.org/abs/2207.05742">arXiv</a>
/
    <a href="https://github.com/ml-jku/reactive-exploration">code</a>
    <p></p>
    <p>We propose Reactive Exploration to track and react to continual domain shifts in lifelong reinforcement learning, and to update the policy correspondingly. 
    </p>
  </td>
    </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/offlinerl.png' width="160">
      </td>
    
      <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2111.04714">
      <papertitle>A Dataset Perspective on Offline Reinforcement Learning</papertitle>
    </a>
    <br>
    Kajetan Schweighofer,
    Andreas Radler,
    <a href="https://scholar.google.com/citations?user=vh39WvYAAAAJ&hl=en">Marius-Constantin Dinu</a>,
    <a href="https://scholar.google.com/citations?user=FD27EMIAAAAJ&hl=en">Markus Hofmarcher</a>,
    <strong>Vihang Patil</strong>,
    <a href="https://scholar.google.com/citations?user=6f4h-xgAAAAJ&hl=en">Angela Bitto-Nemling</a>,
    <a href="https://scholar.google.com/citations?user=-yGxzA4AAAAJ&hl=en">Hamid Eghbal-Zadeh</a>,
    <a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Sepp Hochreiter</a>
    <br>
    <em>Conference on Lifelong Learning (COLLAS)</em>, 2022
    <br>
    <a href="https://arxiv.org/abs/2111.04714">arxiv</a>
    /
    <a href="https://github.com/ml-jku/OfflineRL">code</a>
    <p></p>
    <p>We conducted a comprehensive empirical analysis of how dataset characteristics effect the performance of Offline RL algorithms for discrete action environments.
    </p>
    </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/pcces.png' width="160">
      </td>
    
      <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://proceedings.mlr.press/v151/diouane22a.html">
      <papertitle>A Globally Convergent Evolutionary Strategy for Stochastic Constrained Optimization with Applications to Reinforcement Learning</papertitle>
    </a>
    <br>
    <a href="https://scholar.google.com/citations?user=TaYmtf0AAAAJ&hl=en">Youssef Diouane</a>*,
    <a href="https://scholar.google.com/citations?user=V1ONSgIAAAAJ&hl=en">Aurelien Lucchi</a>*,
    <strong>Vihang Patil</strong>*
    <br>
    <em>International Conference on Artificial Intelligence and Statistics (AISTATS)</em>, 2022
    <br>
    <a href="https://proceedings.mlr.press/v151/diouane22a.html">arXiv</a>
    <p></p>
    <p>In this work, we design a novel optimization algorithm with a sufficient decrease mechanism that ensures convergence and that is based only on estimates of the functions. We demonstrate the applicability of this algorithm on two types of experiments: i) a control task for maximizing rewards and ii) maximizing rewards subject to a non-relaxable set of constraints. (*equal contribution)
    </p>
    </td>
    </tr>





<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src='images/mhnrudder.png' width="160">
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
<a href="https://offline-rl-neurips.github.io/2021/pdf/15.pdf">
  <papertitle>Modern Hopfield Networks for Sample-Efficient Return Decomposition from Demonstrations</papertitle>
</a>
<br>
<a href="https://scholar.google.com/citations?user=eMcniXgAAAAJ&hl=en">Michael Widrich</a>,
<a href="https://scholar.google.com/citations?user=FD27EMIAAAAJ&hl=en">Markus Hofmarcher</a>,
<strong>Vihang Patil</strong>,
<a href="https://scholar.google.com/citations?user=6f4h-xgAAAAJ&hl=en">Angela Bitto-Nemling</a>,
<a href="https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en">Sepp Hochreiter</a>
<br>
<em>Offline RL Workshop, Neurips</em>, 2021
<br>
<a href="https://www.youtube.com/watch?v=RXCWyWpg5YE">video</a>
<p></p>
<p>We introduce modern Hopfield networks for return decomposition for delayed rewards (Hopfield-RUDDER). We experimentally show that Hopfield-RUDDER is able to outperform LSTM-based RUDDER on various 1D environments with small numbers of episodes. 
</p>
</td>
</tr>



  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src='images/ges.png' width="160">
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
  <a href="https://thesis.bul.sbu.usi.ch/theses/1725-1819Patil/pdf?1574252153">
    <papertitle>Guided Search for Maximum Entropy Reinforcement Learning</papertitle>
  </a>
  <br>
  <strong>Vihang Patil</strong>
  <br>
 2019
  <br>
  <p></p>
  <p>We propose a new convergent hybrid method which utilizes policy gradient directions to search in a smaller sub-space, called Guided Evolution Strategies with Sufficient Increase.
  </p>
</td>
  </tr>

</tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <p>
                Modified from Jon Barron's <a href="https://jonbarron.info/"> website.</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
